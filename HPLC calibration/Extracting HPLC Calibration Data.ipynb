{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codes for extracting peak areas from .D files from HPLC calibration runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Extracting HPLC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data_wl(directory, wl):\n",
    "    cols  = [\"No.\", \"rt\", \"type\", \"peak\", \"area\", \"height\", \"%area\"]\n",
    "    cols2 = [\"File\", \"RT IS\", \"RT analyte\", f\"IS int {wl}\", f\"A int {wl}\"]\n",
    "\n",
    "    data_ = []\n",
    "\n",
    "    # --- PASS 1: reference folder containing '50' ---\n",
    "    report_no = None\n",
    "    first = None\n",
    "\n",
    "    # Only consider actual folders\n",
    "    folders = [f for f in os.listdir(directory) if os.path.isdir(os.path.join(directory, f))]\n",
    "\n",
    "    for folder in folders:\n",
    "        if \"50\" not in folder:\n",
    "            continue\n",
    "\n",
    "        folder_path = os.path.join(directory, folder)\n",
    "        report00 = os.path.join(folder_path, \"REPORT00.csv\")\n",
    "        if not os.path.exists(report00):\n",
    "            continue\n",
    "\n",
    "        # Match your working function's encoding\n",
    "        dat = pd.read_table(report00, encoding=\"utf-16\", header=None)\n",
    "        q = dat.columns[0]\n",
    "\n",
    "        # Find \"Number of Signals\"\n",
    "        index0 = None\n",
    "        for i in range(len(dat[q])):\n",
    "            line = dat[q].iloc[i]\n",
    "            if pd.isna(line):\n",
    "                continue\n",
    "            s = str(line)\n",
    "            if s.startswith(\"Number of Signals\"):\n",
    "                index0 = i + 1\n",
    "                break\n",
    "\n",
    "        if index0 is None:\n",
    "            raise ValueError(f\"Could not find 'Number of Signals' in {report00}\")\n",
    "\n",
    "        # Advance until wl appears\n",
    "        while index0 < len(dat[q]) and str(wl) not in str(dat[q].iloc[index0]):\n",
    "            index0 += 1\n",
    "\n",
    "        if index0 >= len(dat[q]):\n",
    "            raise ValueError(f\"No data for wavelength {wl} in {report00}\")\n",
    "\n",
    "        line = str(dat[q].iloc[index0])\n",
    "\n",
    "        # Vendor-format specific: still using fixed position, but validate\n",
    "        report_no = line[7].strip()\n",
    "        file_name = os.path.join(folder_path, f\"REPORT0{report_no}.CSV\")\n",
    "        if not os.path.exists(file_name):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Computed report file does not exist: {file_name}\\n\"\n",
    "                f\"(Parsed report_no='{report_no}' from line: {line!r})\"\n",
    "            )\n",
    "\n",
    "        data = pd.read_csv(file_name, names=cols, encoding=\"utf-16\")\n",
    "\n",
    "        # Force numeric\n",
    "        data[\"area\"] = pd.to_numeric(data[\"area\"], errors=\"coerce\")\n",
    "        data[\"rt\"]   = pd.to_numeric(data[\"rt\"],   errors=\"coerce\")\n",
    "        data = data.dropna(subset=[\"area\", \"rt\"])\n",
    "\n",
    "        if len(data) < 2:\n",
    "            raise ValueError(f\"Reference file {file_name} has fewer than 2 valid peaks.\")\n",
    "\n",
    "        # Top 2 by area\n",
    "        top2 = data.nlargest(2, \"area\").reset_index(drop=True)\n",
    "\n",
    "        rt_is = float(top2.loc[0, \"rt\"])\n",
    "        rt_a  = float(top2.loc[1, \"rt\"])\n",
    "        a_is  = float(top2.loc[0, \"area\"])\n",
    "        a_a   = float(top2.loc[1, \"area\"])\n",
    "\n",
    "        first = [folder, rt_is, rt_a, a_is, a_a]\n",
    "        data_.append(first)\n",
    "        break\n",
    "\n",
    "    if report_no is None or first is None:\n",
    "        raise ValueError(\"No reference folder containing '50' was found (or reference parsing failed).\")\n",
    "\n",
    "    # --- PASS 2: process other folders ---\n",
    "    for folder in folders:\n",
    "        if \"50\" in folder:\n",
    "            continue\n",
    "        if len(folder) < 5:\n",
    "            continue\n",
    "\n",
    "        folder_path = os.path.join(directory, folder)\n",
    "        file_name = os.path.join(folder_path, f\"REPORT0{report_no}.CSV\")\n",
    "        if not os.path.exists(file_name):\n",
    "            continue\n",
    "\n",
    "        data = pd.read_csv(file_name, names=cols, encoding=\"utf-16\")\n",
    "        data[\"area\"] = pd.to_numeric(data[\"area\"], errors=\"coerce\")\n",
    "        data[\"rt\"]   = pd.to_numeric(data[\"rt\"],   errors=\"coerce\")\n",
    "        data = data.dropna(subset=[\"area\", \"rt\"])\n",
    "\n",
    "        if len(data) < 2:\n",
    "            continue\n",
    "\n",
    "        # Get top peaks by area (descending)\n",
    "        data_sorted = data.sort_values(\"area\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "        # Candidate IS/analyte from top 2\n",
    "        rt_  = float(data_sorted.loc[0, \"rt\"])\n",
    "        rt_2 = float(data_sorted.loc[1, \"rt\"])\n",
    "        max_  = float(data_sorted.loc[0, \"area\"])\n",
    "        max_2 = float(data_sorted.loc[1, \"area\"])\n",
    "\n",
    "        # If >2 peaks, try to find analyte RT within Â±10% of reference analyte RT\n",
    "        if len(data_sorted) > 2 and not pd.isna(first[2]):\n",
    "            ref_rt = float(first[2])\n",
    "            if ref_rt != 0:\n",
    "                found = False\n",
    "                for k in range(1, len(data_sorted)):  # scan candidates (by area rank)\n",
    "                    cand_rt = float(data_sorted.loc[k, \"rt\"])\n",
    "                    ratio = cand_rt / ref_rt\n",
    "                    if 0.9 <= ratio <= 1.1:\n",
    "                        rt_2 = cand_rt\n",
    "                        max_2 = float(data_sorted.loc[k, \"area\"])\n",
    "                        found = True\n",
    "                        break\n",
    "                if not found:\n",
    "                    rt_2 = np.nan\n",
    "                    max_2 = np.nan\n",
    "\n",
    "        # If identical RTs, pick next candidate for IS peak (if possible)\n",
    "        if rt_ == rt_2 and len(data_sorted) >= 3:\n",
    "            rt_  = float(data_sorted.loc[1, \"rt\"])\n",
    "            max_ = float(data_sorted.loc[1, \"area\"])\n",
    "\n",
    "        data_.append([folder, rt_, rt_2, max_, max_2])\n",
    "\n",
    "    # Build output table safely\n",
    "    table = pd.DataFrame(data_, columns=cols2)\n",
    "\n",
    "    # Robust numeric conversion for sorting/writing\n",
    "    table[f\"A int {wl}\"] = pd.to_numeric(table[f\"A int {wl}\"], errors=\"coerce\")\n",
    "\n",
    "    table2 = table.sort_values(by=f\"A int {wl}\", ascending=False)\n",
    "    table2.to_csv(f\"{directory}_largest_areas_{wl}.csv\", index=False)\n",
    "\n",
    "    return data_, table2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data_several(directory, no, sens, filename=None):\n",
    "    if filename is None:\n",
    "        filename = f\"{directory}_calibration_data.xlsx\"\n",
    "\n",
    "    writer = pd.ExcelWriter(filename, engine=\"xlsxwriter\")\n",
    "    datas = []\n",
    "\n",
    "    # --- pick a folder that actually is a directory ---\n",
    "    entries = os.listdir(directory)\n",
    "    folders = [f for f in entries if os.path.isdir(os.path.join(directory, f))]\n",
    "    if not folders:\n",
    "        writer.close()\n",
    "        raise ValueError(f\"No subfolders found in directory: {directory}\")\n",
    "\n",
    "    folder1 = folders[0]\n",
    "    report00 = os.path.join(directory, folder1, \"REPORT00.csv\")\n",
    "    if not os.path.exists(report00):\n",
    "        writer.close()\n",
    "        raise FileNotFoundError(f\"Missing REPORT00.csv at: {report00}\")\n",
    "\n",
    "    dat = pd.read_table(report00, encoding=\"utf-16\", header=None)\n",
    "    q = dat.columns[0]\n",
    "\n",
    "    # --- find \"Number of Signals\" safely ---\n",
    "    index0 = None\n",
    "    for i in range(len(dat[q])):\n",
    "        line = dat[q].iloc[i]\n",
    "        if pd.isna(line):\n",
    "            continue\n",
    "        s = str(line)\n",
    "        if s.startswith(\"Number of Signals\"):\n",
    "            index0 = i + 1\n",
    "            break\n",
    "\n",
    "    if index0 is None:\n",
    "        writer.close()\n",
    "        raise ValueError(f'\"Number of Signals\" not found in {report00}')\n",
    "\n",
    "    # --- parse signal lines with bounds check ---\n",
    "    wls, nos = [], []\n",
    "    while index0 < len(dat[q]):\n",
    "        line = dat[q].iloc[index0]\n",
    "        if pd.isna(line):\n",
    "            index0 += 1\n",
    "            continue\n",
    "        s = str(line)\n",
    "        if not s or s[0] != \"S\":\n",
    "            break\n",
    "\n",
    "        wl = s[22:25]\n",
    "        if wl not in wls:\n",
    "            wls.append(wl)\n",
    "            # original logic: report number is one char at pos 7\n",
    "            nos.append(s[7])\n",
    "        index0 += 1\n",
    "\n",
    "    for i in range(len(wls)):\n",
    "        cols = [\"No.\", \"rt\", \"type\", \"peak\", \"area\", \"height\", \"%area\"]\n",
    "\n",
    "        cols2 = [\"File\", \"RT IS\"]\n",
    "        for k in range(no):\n",
    "            cols2.append(f\"RT analyte {k+1}\")\n",
    "        cols2.append(f\"IS int at {wls[i]}\")\n",
    "        for l in range(no):\n",
    "            cols2.append(f\"A int analyte {l+1} at {wls[i]}\")\n",
    "\n",
    "        data_ = []\n",
    "\n",
    "        # --- PASS 1: reference (folders containing '60') ---\n",
    "        rts1 = None\n",
    "        for folder in os.listdir(directory):\n",
    "            if \"60\" not in folder:\n",
    "                continue\n",
    "            if not os.path.isdir(os.path.join(directory, folder)):\n",
    "                continue\n",
    "\n",
    "            file_name = os.path.join(directory, folder, f\"REPORT0{nos[i]}.CSV\")\n",
    "            if not os.path.exists(file_name):\n",
    "                continue\n",
    "\n",
    "            data = pd.read_csv(file_name, names=cols, encoding=\"utf-16\")\n",
    "            list_data = list(data[\"area\"])\n",
    "            list_data_2 = list(data[\"area\"])\n",
    "            list_data.sort(reverse=True)\n",
    "\n",
    "            maxs1 = list_data[: (no + 1)]\n",
    "            rts1 = []\n",
    "            for a in maxs1:\n",
    "                ind = list_data_2.index(a)\n",
    "                rts1.append(data[\"rt\"].iloc[ind])\n",
    "\n",
    "            values = [folder]\n",
    "            while len(rts1) < no + 1:\n",
    "                rts1.append(np.nan)\n",
    "                maxs1.append(np.nan)\n",
    "\n",
    "            values.extend(rts1)\n",
    "            values.extend(maxs1)\n",
    "            data_.append(values)\n",
    "\n",
    "        # --- PASS 2: other folders ---\n",
    "        for folder in os.listdir(directory):\n",
    "            if not os.path.isdir(os.path.join(directory, folder)):\n",
    "                continue\n",
    "            if \"60\" in folder:\n",
    "                continue\n",
    "            if len(folder) < 5 and \"B\" in folder:\n",
    "                continue\n",
    "            if \"blank\" in folder:\n",
    "                continue\n",
    "\n",
    "            file_name = os.path.join(directory, folder, f\"REPORT0{nos[i]}.CSV\")\n",
    "            if not os.path.exists(file_name):\n",
    "                continue\n",
    "\n",
    "            data = pd.read_csv(file_name, names=cols, encoding=\"utf-16\")\n",
    "            if \"area\" not in data.columns or len(data) == 0:\n",
    "                continue\n",
    "\n",
    "            list_data = list(data[\"area\"])\n",
    "            list_data_2 = list(data[\"area\"])\n",
    "            list_data.sort(reverse=True)\n",
    "\n",
    "            if len(list_data) <= 1:\n",
    "                continue\n",
    "\n",
    "            # RTs corresponding to sorted areas\n",
    "            rt_full = []\n",
    "            for a in list_data:\n",
    "                ind = list_data_2.index(a)\n",
    "                rt_full.append(data[\"rt\"].iloc[ind])\n",
    "\n",
    "            new_rts, new_maxs = [], []\n",
    "            for h in range(no + 1):\n",
    "                if rts1 is None or h >= len(rts1) or h >= len(rt_full):\n",
    "                    new_rts.append(np.nan)\n",
    "                    new_maxs.append(np.nan)\n",
    "                    continue\n",
    "\n",
    "                # candidate at rank h\n",
    "                cand_rt = rt_full[h]\n",
    "                cand_area = list_data[h]\n",
    "\n",
    "                cnt = 0\n",
    "                while True:\n",
    "                    # if reference RT is nan, stop trying to match\n",
    "                    if pd.isna(rts1[h]) or pd.isna(cand_rt):\n",
    "                        break\n",
    "\n",
    "                    ratio = cand_rt / rts1[h]\n",
    "                    if (1 - sens) <= ratio <= (1 + sens):\n",
    "                        break\n",
    "\n",
    "                    cnt += 1\n",
    "                    if cnt >= len(rt_full):\n",
    "                        cand_rt = np.nan\n",
    "                        cand_area = np.nan\n",
    "                        break\n",
    "\n",
    "                    cand_rt = rt_full[cnt]\n",
    "                    cand_area = list_data[cnt]\n",
    "\n",
    "                new_rts.append(cand_rt)\n",
    "                new_maxs.append(cand_area)\n",
    "\n",
    "            row = [folder] + new_rts + new_maxs\n",
    "            data_.append(row)\n",
    "\n",
    "        table = pd.DataFrame(np.array(data_, dtype=object), columns=cols2)\n",
    "\n",
    "        # robust numeric conversion\n",
    "        for w in range(no):\n",
    "            col = f\"A int analyte {w+1} at {wls[i]}\"\n",
    "            table[col] = pd.to_numeric(table[col], errors=\"coerce\")\n",
    "\n",
    "        table2 = table.sort_values(by=f\"A int analyte 1 at {wls[i]}\", ascending=False)\n",
    "        table2.to_excel(writer, sheet_name=f\"Cal data {wls[i]}\")\n",
    "        datas.append(table2)\n",
    "\n",
    "    writer.close()\n",
    "    return datas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excecution for example data (1a and 3aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4SO2Me_1a_3aa=load_csv_data_several('4SO2MeArOPh 1a and 3aa calibration',7,0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4SO2Me_1a_3aa_254=load_csv_data_wl('4SO2MeArOPh 1a and 3aa calibration',254)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
